# HTTP Protocol Usage Analyzer

This project analyzes the adoption of **HTTP/1.1**, **HTTP/2**, and **HTTP/3** among the top 50 most visited websites.  
It does so by automatically visiting each website using Selenium while `tshark` captures live network traffic, then summarizes the negotiated protocol versions.

---

## 📂 Project Structure
```markdown
├── main.py
├── csvReader.py
├── tsInitializer.py
├── dataVisualizer.py
├── data/
│   └── mostVisited50.csv
└── captures/
    ├── output.csv
    └── protocol_distribution.png / .pdf
```
---

## ⚙️ Execution Flow

1. **Run `main.py`**  
   - Starts `tshark` to listen for live network traffic.  
   - Uses Selenium to sequentially visit each domain listed in `mostVisited50.csv`.  
   - Saves captured packets to `captures/output.csv`.

2. **Run `dataVisualizer.py`**  
   - Reads the CSV generated by `tshark`.  
   - Extracts the negotiated protocol version from each row.  
   - Computes their percentage distribution.  
   - Generates a ring (donut) chart and saves it as both `.png` and `.pdf`.

---

## 🧩 File Overview

### `main.py`
Coordinates the entire process:
- Imports required functions from other modules.
- Starts a background `tshark` capture using `run_tshark_continuous()`.
- Sequentially visits each website with `visit_domain()`.
- Waits for all captures to finish before exiting.

### `csvReader.py`
Handles input data:
- Reads the list of domains from `data/mostVisited50.csv`.
- Cleans the entries and returns them as a Python list.
- Skips headers automatically if present.

### `tsInitializer.py`
Manages network capture and browser automation:
- **`run_tshark_continuous()`**  
  Launches `tshark` for a predefined duration, capturing traffic on ports 80 and 443.  
  Exports relevant fields such as the TLS server name, negotiated ALPN string, and HTTP host to a CSV file.
- **`visit_domain()`**  
  Opens each domain in a headless Chrome browser for a few seconds.  
  Starts from a blank page to avoid default Chrome connections (e.g., `google.com`).  
  Closes the browser before moving to the next domain.

### `dataVisualizer.py`
Processes and visualizes the captured data:
- **`load_rows()`**  
  Reads `output.csv`, skips empty and header lines, and returns all valid rows.
- **`extract_protocols()`**  
  Scans each row and counts occurrences of `h3`, `h2`, and `http/1.1`, assigning the highest version found when multiple protocols appear in a single entry.
- **`plot_distribution()`**  
  Converts protocol counts to percentages, prints a summary to console, and plots a ring chart.  
  Saves the result in both `.png` and `.pdf` formats under `captures/`.
- **`analyze_protocol_distribution()`**  
  Runs the full analysis pipeline — loads, counts, and visualizes data.

---

## 📊 Dataset

The dataset of the top 50 most visited websites was retrieved from  
**[SimilarWeb - Top Websites](https://www.similarweb.com/top-websites/)** (free trial version).

---

## 📈 Results

After executing `main.py` and then `dataVisualizer.py`, the captured data was analyzed and visualized.

The chart below shows the distribution of negotiated HTTP protocols (`HTTP/1.1`, `HTTP/2`, and `HTTP/3`) across the top 50 most visited websites.

**Final Visualization:**

![Protocol Distribution Chart](captures/protocol_distribution.pdf)

The percentages shown in the chart represent the relative adoption of each protocol during the test session.  
Minor variations may occur depending on the duration of each capture and the intensity of background network traffic.

---

## 🧠 Notes

- The reliability of results depends on:
  - The **visit duration** per website (longer durations produce more complete captures).  
  - The **total tshark listening time**.  
  - The **amount of noise** from system-level traffic (e.g., background iCloud or Chrome connections).

- After executing `main.py`, ensure that `captures/output.csv` exists, then run `dataVisualizer.py` to produce the charts.
